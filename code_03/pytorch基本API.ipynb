{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用pip安装PyTorch\n",
    "# pip install torch torchvision\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个未初始化的张量\n",
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9926, 0.7192, 0.1602],\n",
      "        [0.8161, 0.2519, 0.4422],\n",
      "        [0.9083, 0.7860, 0.6426],\n",
      "        [0.2171, 0.1131, 0.2514],\n",
      "        [0.1167, 0.1004, 0.4975]])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个随机初始化的张量\n",
    "y = torch.rand(5, 3)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个全零矩阵\n",
    "z = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1., -1.],\n",
      "        [ 2., -2.]])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个直接从数据列表构造的张量\n",
    "w = torch.tensor([[1.0, -1.0], [2.0, -2.0]])\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# 张量的属性\n",
    "print(w.size()) # .size() 已经被弃用，建议使用 .shape\n",
    "print(w.shape) # 推荐的方式\n",
    "print(w.dtype) # 数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.])\n",
      "tensor([5., 7., 9.])\n"
     ]
    }
   ],
   "source": [
    "# 张量操作\n",
    "a = torch.tensor([1., 2., 3.])\n",
    "b = torch.tensor([4., 5., 6.])\n",
    "print(a + b) # 加法\n",
    "print(torch.add(a, b)) # 加法的另一种方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 使用NumPy数组创建张量\n",
    "import numpy as np\n",
    "np_array = np.array([[1, 2], [3, 4.]])\n",
    "t = torch.from_numpy(np_array)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:\n",
      "tensor([[0.1430, 0.2601, 0.0692],\n",
      "        [0.0160, 0.5036, 0.7079]], requires_grad=True)\n",
      "Error: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.\n",
      "Converted to NumPy array with detach and moved to CPU:\n",
      "[[0.14295101 0.26011157 0.06915587]\n",
      " [0.01603687 0.5035888  0.707906  ]]\n"
     ]
    }
   ],
   "source": [
    "# 创建一个随机张量\n",
    "v1 = torch.rand(2, 3, requires_grad=True)\n",
    "print(\"Original Tensor:\")\n",
    "print(v1)\n",
    "\n",
    "# 尝试直接转换为 NumPy 数组（如果v1在GPU上或涉及到梯度计算，这行会报错）\n",
    "try:\n",
    "    np_v1 = v1.numpy()\n",
    "    print(\"Converted to NumPy array without detach:\")\n",
    "    print(np_v1)\n",
    "except RuntimeError as e:\n",
    "    print(\"Error:\", e)\n",
    "\n",
    "# 正确的方式，确保张量不在计算图中且在CPU上\n",
    "np_v1 = v1.detach().cpu().numpy()\n",
    "print(\"Converted to NumPy array with detach and moved to CPU:\")\n",
    "print(np_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自动计算梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)  # 告诉 PyTorch 需要计算这个张量的梯度\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "out = z.mean()  # 计算张量 z 的所有元素的平均值，得到一个标量\n",
    "\n",
    "out.backward() # 计算梯度\n",
    "\n",
    "print(x.grad) # 输出梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络模块（nn.Module）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n",
      "Input:\n",
      " tensor([[0.4856, 0.7791]])\n",
      "Output:\n",
      " tensor([[0.]], grad_fn=<ReluBackward0>)\n",
      "==============================\n",
      "Layer: fc1.weight, Param: Parameter containing:\n",
      "tensor([[ 0.4339, -0.4089]], requires_grad=True), Requires_grad: True\n",
      "==============================\n",
      "Layer: fc1.bias, Param: Parameter containing:\n",
      "tensor([0.0808], requires_grad=True), Requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "# 打印网络结构\n",
    "print(net)\n",
    "\n",
    "# 输入一个随机的张量\n",
    "input = torch.randn(1, 2)\n",
    "print(\"Input:\\n\", input)\n",
    "\n",
    "# 获取网络的输出\n",
    "output = net(input)\n",
    "print(\"Output:\\n\", output)\n",
    "\n",
    "# 打印所有的参数\n",
    "for name, param in net.named_parameters():\n",
    "    print (\"=\"*30)\n",
    "    print(f\"Layer: {name}, Param: {param}, Requires_grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一个隐藏层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (fc2): Linear(in_features=3, out_features=2, bias=True)\n",
      ")\n",
      "Input:\n",
      " tensor([[-0.4912,  0.3828]])\n",
      "Output:\n",
      " tensor([[-0.3606,  0.3850]], grad_fn=<AddmmBackward0>)\n",
      "==============================\n",
      "Layer: fc1.weight, Param: Parameter containing:\n",
      "tensor([[ 0.3640,  0.3488],\n",
      "        [-0.3150, -0.2339],\n",
      "        [ 0.7009, -0.3358]], requires_grad=True), Requires_grad: True\n",
      "==============================\n",
      "Layer: fc1.bias, Param: Parameter containing:\n",
      "tensor([ 0.6362, -0.0496, -0.3305], requires_grad=True), Requires_grad: True\n",
      "==============================\n",
      "Layer: fc2.weight, Param: Parameter containing:\n",
      "tensor([[-0.5282, -0.0131,  0.3920],\n",
      "        [ 0.1203, -0.1582, -0.2640]], requires_grad=True), Requires_grad: True\n",
      "==============================\n",
      "Layer: fc2.bias, Param: Parameter containing:\n",
      "tensor([-0.0482,  0.3163], requires_grad=True), Requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 修改后的网络结构\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 添加一个隐藏层，输入特征为2，隐藏层特征为3\n",
    "        self.fc1 = nn.Linear(2, 3)\n",
    "        # 添加一个输出层，隐藏层特征为5，输出特征为3\n",
    "        self.fc2 = nn.Linear(3, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 使用ReLU激活函数处理隐藏层的输出\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # 处理输出层的输出\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 实例化网络\n",
    "net = Net()\n",
    "\n",
    "# 打印网络结构\n",
    "print(net)\n",
    "\n",
    "# 输入一个随机的张量\n",
    "input = torch.randn(1, 2)\n",
    "print(\"Input:\\n\", input)\n",
    "\n",
    "# 获取网络的输出\n",
    "output = net(input)\n",
    "print(\"Output:\\n\", output)\n",
    "\n",
    "# 打印所有的参数\n",
    "for name, param in net.named_parameters():\n",
    "    print (\"=\"*30)\n",
    "    print(f\"Layer: {name}, Param: {param}, Requires_grad: {param.requires_grad}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化器（Optimizer）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " tensor([[ 0.9711, -0.6973]])\n",
      "Target:\n",
      " tensor([[-0.2845]])\n",
      "Output before backward pass:\n",
      " tensor([[0.1210]], grad_fn=<ReluBackward0>)\n",
      "Loss:\n",
      " 0.16444812715053558\n",
      "Output after one step of gradient descent:\n",
      " tensor([[0.1013]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义网络\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 1)  # 输入特征为2，输出特征为1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "# 实例化网络\n",
    "net = Net()\n",
    "\n",
    "# 随机生成输入数据\n",
    "input = torch.randn(1, 2)  # 一个样本，两个特征\n",
    "print(\"Input:\\n\", input)\n",
    "\n",
    "# 随机生成目标数据（对于回归问题）\n",
    "target = torch.randn(1, 1)  # 一个样本，一个特征（假设）\n",
    "print(\"Target:\\n\", target)\n",
    "\n",
    "# 或者如果是分类问题，目标可以是这样的：\n",
    "# target = torch.tensor([0])  # 假设只有两个类别，这里的目标是第一个类别\n",
    "\n",
    "# 定义损失函数\n",
    "criterion = nn.MSELoss()  # 均方误差损失函数\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# 清空梯度缓存\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# 前向传播\n",
    "output = net(input)\n",
    "print(\"Output before backward pass:\\n\", output)\n",
    "\n",
    "# 计算损失\n",
    "loss = criterion(output, target)\n",
    "print(\"Loss:\\n\", loss.item())\n",
    "\n",
    "# 反向传播\n",
    "loss.backward()\n",
    "\n",
    "# 更新参数\n",
    "optimizer.step()\n",
    "\n",
    "# 再次前向传播，查看更新后的输出\n",
    "output_after_update = net(input)\n",
    "print(\"Output after one step of gradient descent:\\n\", output_after_update)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ait",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
